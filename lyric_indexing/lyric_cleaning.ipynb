{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7da78ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/cep4u/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import contractions\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "import gensim\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "935445ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 CSV files extracted.\n",
      "Total: 149445\n"
     ]
    }
   ],
   "source": [
    "def _get_csv_file_list(unzipping_output_folder):\n",
    "    \"\"\"\n",
    "    Extract all the csv file paths given the folder path.\n",
    "    :param unzipping_output_folder: Folder path.\n",
    "    :return: List of CSV file paths.\n",
    "    \"\"\"\n",
    "    csv_file_list = [i for i in glob.glob(f'{unzipping_output_folder}/**/*.csv', recursive=True)]\n",
    "    print(f'{len(csv_file_list)} CSV files extracted.')\n",
    "    return csv_file_list\n",
    "\n",
    "def _read_and_combine(csv_file_list):\n",
    "    \"\"\"\n",
    "    Read the list of data CSVs and combine them into a single DataFrame.\n",
    "    Handles CSVs where from the 5th column onwards, data should be merged into one column.\n",
    "    :param csv_file_list: List of CSV file paths where the data is.\n",
    "    :return: Combined DataFrame.\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    \n",
    "    for csv_file_name in csv_file_list:\n",
    "        with open(csv_file_name, 'r') as file:\n",
    "            header = next(file).strip().split(\",\")[:4] + [\"lyrics\"]\n",
    "            for line in file:\n",
    "                fields = line.strip().split(\",\")\n",
    "                \n",
    "                merged_field = \" \".join(fields[4:])\n",
    "                all_data.append(fields[:4] + [merged_field])\n",
    "                \n",
    "    combined_df = pd.DataFrame(all_data, columns=header)\n",
    "    print(f'Total: {len(combined_df)}')\n",
    "    return combined_df\n",
    "            \n",
    "data_folder = '/Users/cep4u/JingEdward/tunen/data/lyric_rawdata/azlyrics-scraper'\n",
    "csv_file_list = _get_csv_file_list(data_folder)\n",
    "all_data = _read_and_combine(csv_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bb7e574",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCleaner:\n",
    "    def __init__(self, df, column_name, stop_words, wnl ):\n",
    "        self.df = df\n",
    "        self.column_name = column_name\n",
    "        self.stop_words = stop_words\n",
    "        self.wnl = wnl\n",
    "    \n",
    "    def remove_null(self, df, column_name):\n",
    "        df = df[df[column_name].notnull()]\n",
    "        return df\n",
    "\n",
    "    def remove_contractions(self, df, column_name):\n",
    "        df[f'RemoveContractions_{column_name}'] = df[column_name].apply(lambda x: [contractions.fix(word) for word in x.split()])\n",
    "        return df\n",
    "\n",
    "    def rebuild_string(self, df, column_name):\n",
    "        df[f'{column_name}_string_nocont'] = [' '.join(map(str, l)) for l in df[f'RemoveContractions_{column_name}']]\n",
    "        return df\n",
    "\n",
    "    def tokenize(self, df, column_name):\n",
    "        df[f'tokenized_{column_name}'] = df[f'{column_name}_string_nocont'].apply(word_tokenize)\n",
    "        return df\n",
    "    \n",
    "    def token_cleanup(self, df, column_name):\n",
    "        edge_cases = [\"``\", \"â€™\", \"''\", \"image\", \"title\", \"alt\", \"src\", \"width\", \"img\", \"http\", \"cbc\", \"jpg\", \"16x9_460\", \"buzzfeed\", \"com\", \"h1\", \"href\", \"href=\", 'p', '/p', '/a' \"rel\", \"www\", \"reuters\", \"timesofindia\", \"indiatimes\", \"margin\", \"nofollow\", '8217', '8230']\n",
    "        df[f'tokenized_{column_name}'] = df[f'tokenized_{column_name}'].apply(lambda x: [word.lower() for word in x])\n",
    "        df[f'tokenized_{column_name}'] = df[f'tokenized_{column_name}'].apply(lambda x: [word for word in x if word not in string.punctuation])\n",
    "        df[f'tokenized_{column_name}'] = df[f'tokenized_{column_name}'].apply(lambda x: [word for word in x if word not in self.stop_words])\n",
    "        df[f'tokenized_{column_name}'] = df[f'tokenized_{column_name}'].apply(lambda x: [word for word in x if '/' not in word])\n",
    "        df[f'tokenized_{column_name}'] = df[f'tokenized_{column_name}'].apply(lambda x: [word for word in x if word not in edge_cases])\n",
    "        return df\n",
    "    \n",
    "    def make_bigrams(self, df, column_name):\n",
    "        bigram = gensim.models.Phrases(df[f'tokenized_{column_name}'], min_count=5, threshold=100)\n",
    "        bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "\n",
    "        def get_bigrams(tokens_list):\n",
    "            return bigram_mod[tokens_list]\n",
    "\n",
    "        df[f'bigrams_{column_name}'] = df[f'tokenized_{column_name}'].apply(get_bigrams)\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    def lemmatize_tokens(self, df, column_name):\n",
    "        clean_up = [\"'s\", \"--\"]\n",
    "        df[f'lemmatized_{column_name}'] = df[f'bigrams_{column_name}'].apply(lambda x: [self.wnl.lemmatize(word) for word in x])\n",
    "        df[f'lemmatized_{column_name}'] = df[f'lemmatized_{column_name}'].apply(lambda x: [word for word in x if word not in clean_up])\n",
    "        return df\n",
    "\n",
    "    def clean(self):\n",
    "        df = self.remove_null(self.df, self.column_name)\n",
    "        df = self.remove_contractions(df, self.column_name)\n",
    "        df = self.rebuild_string(df, self.column_name)\n",
    "        df = self.tokenize(df, self.column_name)\n",
    "        df = self.token_cleanup(df, self.column_name)\n",
    "        df = self.make_bigrams(df, self.column_name)\n",
    "        df = self.lemmatize_tokens(df, self.column_name)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e654ce5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "desc_cleaner_news = DataCleaner(all_data, 'lyrics', stop_words, wnl)\n",
    "cleaned_df_desc = desc_cleaner_news.clean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9edd824",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '/Users/cep4u/JingEdward/tunen/data/lyric_rawdata'\n",
    "\n",
    "filename = 'cleaned_lyric_df.csv'\n",
    "\n",
    "file_path = f\"{data_folder}/{filename}\"\n",
    "\n",
    "cleaned_df_desc.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7191f8ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "melodiik",
   "language": "python",
   "name": "melodiik"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
